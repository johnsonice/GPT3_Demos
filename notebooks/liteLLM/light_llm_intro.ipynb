{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lite LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from agents import Agent, Runner\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Access your API key\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "   raise ValueError(\"OPENAI_API_KEY not found in .env file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import completion\n",
    "import pprint\n",
    "import textwrap\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### call openai with formated output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structured Weather Response:\n",
      "{\"location\":\"San Francisco, CA\",\"temperature\":\"64Â°F\",\"conditions\":\"Partly cloudy\",\"humidity\":\"70%\",\"wind\":\"5 mph\",\"summary\":\"A mild day with some clouds.\"}\n"
     ]
    }
   ],
   "source": [
    "# Define Pydantic model for structured weather response\n",
    "class WeatherResponse(BaseModel):\n",
    "    location: str\n",
    "    temperature: Optional[str] = None\n",
    "    conditions: Optional[str] = None\n",
    "    humidity: Optional[str] = None\n",
    "    wind: Optional[str] = None\n",
    "    summary: str\n",
    "\n",
    "## Call 4o mini with structured output\n",
    "messages=[{\"role\": \"system\", \"content\": \"You are a helpful weather assistant. Respond with structured weather information.\"},\n",
    "          {\"role\": \"user\", \"content\": \"what's the weather in SF\"}]\n",
    "\n",
    "response = completion(\n",
    "    messages=messages,\n",
    "    model='gpt-4o-mini',\n",
    "    max_completion_tokens=100,  ## equals to max output tokens\n",
    "    response_format=WeatherResponse  # Use Pydantic model for structured output\n",
    ")\n",
    "\n",
    "# Parse the structured response\n",
    "weather_data = response['choices'][0]['message']['content']\n",
    "print(\"Structured Weather Response:\")\n",
    "print(weather_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### call reasoning models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning Content:\n",
      "Recursion is a fundamental concept in programming where a function calls itself to solve a problem. Let me plan a thorough explanation with a simple example.\n",
      "\n",
      "Key points to cover:\n",
      "1. Definition of recursion\n",
      "2. How recursion works conceptually\n",
      "3. Key components of a recursive function (base case and recursive case)\n",
      "4. A simple example (factorial or fibonacci are common)\n",
      "5. Tracing through the example step by step\n",
      "6. Advantages and disadvantages of recursion\n",
      "7. Common pitfalls and considerations\n",
      "\n",
      "For the example, I'll use calculating factorial, which is a classic and intuitive example of recursion. Factorial of n (written as n!) is the product of all positive integers less than or equal to n.\n",
      "\n",
      "Example: 5! = 5 Ã— 4 Ã— 3 Ã— 2 Ã— 1 = 120\n",
      "\n",
      "This is a good example because:\n",
      "- It has a clear mathematical definition\n",
      "- It can be expressed recursively: n! = n Ã— (n-1)!\n",
      "- It has a simple base case: 0! = 1 or 1! = 1\n",
      "- The recursion depth is manageable for examples\n",
      "\n",
      "Final Answer:\n",
      "# Recursion in Programming\n",
      "\n",
      "Recursion is a powerful programming technique where a function calls its\n"
     ]
    }
   ],
   "source": [
    "## call thinking models \n",
    "# Call o1-mini thinking model to get both reasoning and final answer\n",
    "messages = [{\"role\": \"user\", \"content\": \"Explain the concept of recursion in programming with a simple example.Please think carefully and provide a detailed explanation.\"}]\n",
    "\n",
    "# Make the completion call to o1-mini\n",
    "response = completion(\n",
    "    model=\"anthropic/claude-3-7-sonnet-20250219\",\n",
    "    messages=messages,\n",
    "    reasoning_effort=\"low\", \n",
    "    )\n",
    "# Check if reasoning content exists and print accordingly\n",
    "if response['choices'][0]['message'].get('reasoning_content'):\n",
    "    print(\"Reasoning Content:\")\n",
    "    print(response['choices'][0]['message']['reasoning_content'])\n",
    "    print(\"\\nFinal Answer:\")\n",
    "    print(response.choices[0].message.content[:100])\n",
    "else:\n",
    "    print(\"Raw Response:\")\n",
    "    print(response.choices[0].message.content[:100])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call gogole gemini 2.5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is **Paris**.\n"
     ]
    }
   ],
   "source": [
    "response = completion(\n",
    "  model=\"gemini/gemini-2.5-flash-preview-04-17\",\n",
    "  messages=[{\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n",
    "  thinking={\"type\": \"enabled\", \"budget_tokens\": 1024},\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### call any other openai competable api  [netmind Error]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mQwen/QwQ-32B\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m            \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://api.netmind.ai/inference-api/openai/v1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m            \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetenv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNETMIND_API_KEY\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.10/site-packages/litellm/utils.py:1283\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logging_obj:\n\u001b[1;32m   1280\u001b[0m     logging_obj\u001b[38;5;241m.\u001b[39mfailure_handler(\n\u001b[1;32m   1281\u001b[0m         e, traceback_exception, start_time, end_time\n\u001b[1;32m   1282\u001b[0m     )  \u001b[38;5;66;03m# DO NOT MAKE THREADED - router retry fallback relies on this!\u001b[39;00m\n\u001b[0;32m-> 1283\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.10/site-packages/litellm/utils.py:1161\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1159\u001b[0m         print_verbose(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while checking max token limit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1160\u001b[0m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[0;32m-> 1161\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43moriginal_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1162\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m   1163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.10/site-packages/litellm/main.py:3241\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001b[0m\n\u001b[1;32m   3238\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m   3239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   3240\u001b[0m     \u001b[38;5;66;03m## Map to OpenAI Exception\u001b[39;00m\n\u001b[0;32m-> 3241\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_type(\n\u001b[1;32m   3242\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m   3243\u001b[0m         custom_llm_provider\u001b[38;5;241m=\u001b[39mcustom_llm_provider,\n\u001b[1;32m   3244\u001b[0m         original_exception\u001b[38;5;241m=\u001b[39me,\n\u001b[1;32m   3245\u001b[0m         completion_kwargs\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   3246\u001b[0m         extra_kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m   3247\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.10/site-packages/litellm/main.py:1039\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m     model \u001b[38;5;241m=\u001b[39m deployment_id\n\u001b[1;32m   1038\u001b[0m     custom_llm_provider \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mazure\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1039\u001b[0m model, custom_llm_provider, dynamic_api_key, api_base \u001b[38;5;241m=\u001b[39m \u001b[43mget_llm_provider\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1047\u001b[0m     provider_specific_header \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m provider_specific_header[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom_llm_provider\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m custom_llm_provider\n\u001b[1;32m   1049\u001b[0m ):\n\u001b[1;32m   1050\u001b[0m     headers\u001b[38;5;241m.\u001b[39mupdate(provider_specific_header[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.10/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py:373\u001b[0m, in \u001b[0;36mget_llm_provider\u001b[0;34m(model, custom_llm_provider, api_base, api_key, litellm_params)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, litellm\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mBadRequestError):\n\u001b[0;32m--> 373\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    375\u001b[0m         error_str \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    376\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGetLLMProvider Exception - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124moriginal model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    377\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.10/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py:350\u001b[0m, in \u001b[0;36mget_llm_provider\u001b[0;34m(model, custom_llm_provider, api_base, api_key, litellm_params)\u001b[0m\n\u001b[1;32m    348\u001b[0m     error_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Pass model as E.g. For \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHuggingface\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m inference endpoints pass in `completion(model=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuggingface/starcoder\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,..)` Learn more: https://docs.litellm.ai/docs/providers\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;66;03m# maps to openai.NotFoundError, this is raised when openai does not recognize the llm\u001b[39;00m\n\u001b[0;32m--> 350\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m litellm\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mBadRequestError(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    351\u001b[0m         message\u001b[38;5;241m=\u001b[39merror_str,\n\u001b[1;32m    352\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    353\u001b[0m         response\u001b[38;5;241m=\u001b[39mhttpx\u001b[38;5;241m.\u001b[39mResponse(\n\u001b[1;32m    354\u001b[0m             status_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m400\u001b[39m,\n\u001b[1;32m    355\u001b[0m             content\u001b[38;5;241m=\u001b[39merror_str,\n\u001b[1;32m    356\u001b[0m             request\u001b[38;5;241m=\u001b[39mhttpx\u001b[38;5;241m.\u001b[39mRequest(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompletion\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/BerriAI/litellm\u001b[39m\u001b[38;5;124m\"\u001b[39m),  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    357\u001b[0m         ),\n\u001b[1;32m    358\u001b[0m         llm_provider\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    359\u001b[0m     )\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_base \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(api_base, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[1;32m    362\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi base needs to be a string. api_base=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(api_base)\n\u001b[1;32m    363\u001b[0m     )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers"
     ]
    }
   ],
   "source": [
    "#### there is a problem with server setup \n",
    "response = completion(\n",
    "            model=\"Qwen/QwQ-32B\",\n",
    "            messages=messages,\n",
    "            api_base=\"https://api.netmind.ai/inference-api/openai/v1\",\n",
    "            api_key=os.getenv(\"NETMIND_API_KEY\"),\n",
    "            temperature=0.2,\n",
    "            max_tokens=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning Content:\n",
      "Okay, the user is asking \"what llm are you\". Let me break this down. LLM stands for Large Language Model, so they're asking which one I am.\n",
      "\n",
      "First, I need to confirm that they're referring to my model type. Since I'm Qwen, I should explain that I am a large language model developed by Alibaba Cloud. \n",
      "\n",
      "They might want to know specifics like my architecture, training data, or capabilities. I should mention that I'm based on the Qwen series, which uses advanced transformer architecture. \n",
      "\n",
      "Also, maybe they're curious about my training data. I should state that it's extensive and comes from Alibaba Group's internal historical accumulation, but not specify exact dates or sources due to confidentiality.\n",
      "\n",
      "I should highlight my features, like handling various tasks, multi-language support, code generation, etc. But keep it concise. \n",
      "\n",
      "Wait, should I mention my parameters? The user didn't ask for that specifically, but it's part of being an LLM. Maybe include it briefly. \n",
      "\n",
      "Also, maybe they want to know how I compare to others like GPT or BERT. But the question is straightforward, so perhaps stick to the facts without going into comparisons unless necessary.\n",
      "\n",
      "I should make sure to use simple language and avoid technical jargon as much as possible. \n",
      "\n",
      "Let me structure the response: start by stating I'm Qwen, part of Alibaba Cloud's LLM family. Then mention the architecture, training data, and key capabilities. Offer to provide more details if needed.\n",
      "\n",
      "Content:\n",
      "\n",
      "\n",
      "I am **Qwen**, a large language model developed by Alibaba Cloud. I belong to the **Qwen series**, which includes various versions like Qwen1, Qwen1.5, Qwen2, Qwen2.5, and specialized models such as Qwen-Audio, Qwen-VL, and Qwen-Max. \n",
      "\n",
      "### Key Features:\n",
      "- **Architecture**: Based on the **Transformer** architecture, optimized for natural language understanding and generation.\n",
      "- **Training Data**: Trained on a vast amount of text data from Alibaba Group's internal historical accumulation, up to December 2024 (the cutoff date varies slightly across different versions).\n",
      "- **Capabilities**: \n",
      "  - Answer questions, write stories, emails, scripts, and more.\n",
      "  - Perform logical reasoning, programming, and multi-step tasks.\n",
      "  - Support over **100 languages**, including Chinese, English, French, Spanish, Portuguese, Russian, Arabic, Japanese, Korean, Vietnamese, Thai, Indonesian, etc.\n",
      "  - Generate code, analyze data, and assist with technical tasks (in some versions like Qwen-Max).\n",
      "\n",
      "### Specialized Models:\n",
      "- **Qwen-Audio**: Handles audio-related tasks (e.g., speech-to-text, audio analysis).\n",
      "- **Qwen-VL**: Processes visual and language data (e.g., image captioning, visual reasoning).\n",
      "- **Qwen-Max**: The most capable version for complex, multi-step tasks.\n",
      "\n",
      "Let me know if you'd like details about a specific version or application! ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "client = openai.OpenAI(\n",
    "    api_key=os.getenv(\"NETMIND_API_KEY\"),            # pass litellm proxy key, if you're using virtual keys\n",
    "    base_url=\"https://api.netmind.ai/inference-api/openai/v1\" # litellm-proxy-base url\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"Qwen/QwQ-32B\",\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"what llm are you\"\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "reasoning_content = response.choices[0].message.reasoning_content\n",
    "content = response.choices[0].message.content\n",
    "print('Reasoning Content:')\n",
    "print(reasoning_content)\n",
    "print('Content:')\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other useages \n",
    "- https://docs.litellm.ai/docs/providers/gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
