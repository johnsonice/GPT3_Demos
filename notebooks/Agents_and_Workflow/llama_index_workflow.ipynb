{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Llama Index Workflow Basics and Examples\n",
    "- https://docs.llamaindex.ai/en/stable/examples/workflow/workflows_cookbook/\n",
    "- https://github.com/run-llama/llama_index/tree/main/docs/docs/examples/workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import (\n",
    "    Event,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    step,\n",
    "    Context,\n",
    ")\n",
    "import random\n",
    "from llama_index.utils.workflow import draw_most_recent_execution,draw_all_possible_flows\n",
    "from llama_index.llms.openai import OpenAI\n",
    "#from llama_index.core.workflow import draw_all_possible_flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "env_path = '../../.env'\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "openai_apikey = os.environ[\"OPENAI_API_KEY\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Intro to functionalities \n",
    "    - most basic workflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAIGenerator(Workflow):\n",
    "    @step\n",
    "    async def generate(self, ev: StartEvent) -> StopEvent:\n",
    "        llm = OpenAI(model=\"gpt-4o-mini\",api_key=openai_apikey)\n",
    "        response = await llm.acomplete(ev.query)\n",
    "        return StopEvent(result=str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaIndex, formerly known as GPT Index, is a framework designed to facilitate the integration of large language models (LLMs) with various data sources. It provides tools and structures to help developers create applications that can efficiently query and retrieve information from different types of data, such as documents, databases, and APIs, using natural language processing.\n",
      "\n",
      "The main features of LlamaIndex include:\n",
      "\n",
      "1. **Data Connection**: It allows users to connect to various data sources, making it easier to pull in relevant information for processing.\n",
      "\n",
      "2. **Indexing**: LlamaIndex helps in indexing the data, which optimizes the retrieval process and enhances the performance of queries made to the LLM.\n",
      "\n",
      "3. **Querying**: Users can perform natural language queries against the indexed data, enabling more intuitive interactions with the information.\n",
      "\n",
      "4. **Integration with LLMs**: The framework is designed to work seamlessly with large language models, allowing developers to leverage the capabilities of LLMs in their applications.\n",
      "\n",
      "Overall, LlamaIndex aims to simplify the process of building applications that require sophisticated data retrieval and processing capabilities, making it easier for developers to harness the power of AI in their projects.\n"
     ]
    }
   ],
   "source": [
    "w = OpenAIGenerator(timeout=10, verbose=False)\n",
    "result = await w.run(query=\"What's LlamaIndex?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/home/xiong/data/temp/temp_workflow.html\n"
     ]
    }
   ],
   "source": [
    "## show workflow in a diagram\n",
    "temp_file_name = '/data/home/xiong/data/temp/temp_workflow.html'\n",
    "draw_all_possible_flows(OpenAIGenerator,filename=temp_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Loops and branches\n",
    "- Let's go to a more interesting example, demonstrating our ability to loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FailedEvent(Event):\n",
    "    error: str\n",
    "class QueryEvent(Event):\n",
    "    query: str\n",
    "class LoopExampleFlow(Workflow):\n",
    "    @step\n",
    "    async def answer_query(self, ev: Union[StartEvent,QueryEvent ]) ->  Union[FailedEvent, StopEvent]:  ## it can take start ecent or query event and emmit eigher failedevent or stopevent\n",
    "        query = ev.query\n",
    "        # try to answer the query\n",
    "        random_number = random.randint(0, 1)\n",
    "        if random_number == 0:\n",
    "            return FailedEvent(error=\"Failed to answer the query.\")\n",
    "        else:\n",
    "            return StopEvent(result=\"The answer to your query\")\n",
    "\n",
    "    @step\n",
    "    async def improve_query(self, ev: FailedEvent) ->  Union[QueryEvent, StopEvent]:\n",
    "        # improve the query or decide it can't be fixed\n",
    "        random_number = random.randint(0, 1)\n",
    "        if random_number == 0:\n",
    "            return QueryEvent(query=\"Here's a better query.\")\n",
    "        else:\n",
    "            return StopEvent(result=\"Your query can't be fixed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/home/xiong/data/temp/temp_workflow.html\n"
     ]
    }
   ],
   "source": [
    "draw_all_possible_flows(LoopExampleFlow, filename=temp_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](workflow_loop.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
