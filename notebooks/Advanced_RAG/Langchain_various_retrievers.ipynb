{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Self Querying retriever\n",
    "- https://github.com/samwit/langchain-tutorials/blob/main/RAG/YT_LangChain_RAG_tips_and_Tricks_01_Self_Query.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,getpass\n",
    "import pprint\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(prompt='OpenAI API Token:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/gpt/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. Use langchain_openai.OpenAIEmbeddings instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "embeddings = OpenAIEmbeddings() ## probably want to check which embedding endpoint it is using"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Create some sample documents to work with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    Document(\n",
    "        page_content=\"Complex, layered, rich red with dark fruit flavors\",\n",
    "        metadata={\"name\":\"Opus One\", \"year\": 2018, \"rating\": 96, \"grape\": \"Cabernet Sauvignon\", \"color\":\"red\", \"country\":\"USA\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Luxurious, sweet wine with flavors of honey, apricot, and peach\",\n",
    "        metadata={\"name\":\"Château d'Yquem\", \"year\": 2015, \"rating\": 98, \"grape\": \"Sémillon\", \"color\":\"white\", \"country\":\"France\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Full-bodied red with notes of black fruit and spice\",\n",
    "        metadata={\"name\":\"Penfolds Grange\", \"year\": 2017, \"rating\": 97, \"grape\": \"Shiraz\", \"color\":\"red\", \"country\":\"Australia\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Elegant, balanced red with herbal and berry nuances\",\n",
    "        metadata={\"name\":\"Sassicaia\", \"year\": 2016, \"rating\": 95, \"grape\": \"Cabernet Franc\", \"color\":\"red\", \"country\":\"Italy\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Highly sought-after Pinot Noir with red fruit and earthy notes\",\n",
    "        metadata={\"name\":\"Domaine de la Romanée-Conti\", \"year\": 2018, \"rating\": 100, \"grape\": \"Pinot Noir\", \"color\":\"red\", \"country\":\"France\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Crisp white with tropical fruit and citrus flavors\",\n",
    "        metadata={\"name\":\"Cloudy Bay\", \"year\": 2021, \"rating\": 92, \"grape\": \"Sauvignon Blanc\", \"color\":\"white\", \"country\":\"New Zealand\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Rich, complex Champagne with notes of brioche and citrus\",\n",
    "        metadata={\"name\":\"Krug Grande Cuvée\", \"year\": 2010, \"rating\": 93, \"grape\": \"Chardonnay blend\", \"color\":\"sparkling\", \"country\":\"New Zealand\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Intense, dark fruit flavors with hints of chocolate\",\n",
    "        metadata={\"name\":\"Caymus Special Selection\", \"year\": 2018, \"rating\": 96, \"grape\": \"Cabernet Sauvignon\", \"color\":\"red\", \"country\":\"USA\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Exotic, aromatic white with stone fruit and floral notes\",\n",
    "        metadata={\"name\":\"Jermann Vintage Tunina\", \"year\": 2020, \"rating\": 91, \"grape\": \"Sauvignon Blanc blend\", \"color\":\"white\", \"country\":\"Italy\"},\n",
    "    ),\n",
    "]\n",
    "vectorstore = Chroma.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.base import AttributeInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"grape\",\n",
    "        description=\"The grape used to make the wine\",\n",
    "        type=\"string or list[string]\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"name\",\n",
    "        description=\"The name of the wine\",\n",
    "        type=\"string or list[string]\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"color\",\n",
    "        description=\"The color of the wine\",\n",
    "        type=\"string or list[string]\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"year\",\n",
    "        description=\"The year the wine was released\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"country\",\n",
    "        description=\"The name of the country the wine comes from\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"rating\", description=\"The Robert Parker rating for the wine 0-100\", type=\"integer\" #float\n",
    "    ),\n",
    "]\n",
    "document_content_description = \"Brief description of the wine\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Set up the retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/gpt/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.openai.OpenAI` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. Use langchain_openai.OpenAI instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm,\n",
    "    vectorstore,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    "    verbose=True\n",
    ")\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- it should generate a query for semetic matching and a filter for metadata filtering\n",
    "- see [link](https://python.langchain.com/docs/modules/data_connection/retrievers/self_query/#constructing-from-scratch-with-lcel) to see how to further customize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Crisp white with tropical fruit and citrus flavors', metadata={'color': 'white', 'country': 'New Zealand', 'grape': 'Sauvignon Blanc', 'name': 'Cloudy Bay', 'rating': 92, 'year': 2021}), Document(page_content='Intense, dark fruit flavors with hints of chocolate', metadata={'color': 'red', 'country': 'USA', 'grape': 'Cabernet Sauvignon', 'name': 'Caymus Special Selection', 'rating': 96, 'year': 2018}), Document(page_content='Luxurious, sweet wine with flavors of honey, apricot, and peach', metadata={'color': 'white', 'country': 'France', 'grape': 'Sémillon', 'name': \"Château d'Yquem\", 'rating': 98, 'year': 2015}), Document(page_content='Complex, layered, rich red with dark fruit flavors', metadata={'color': 'red', 'country': 'USA', 'grape': 'Cabernet Sauvignon', 'name': 'Opus One', 'rating': 96, 'year': 2018})]\n"
     ]
    }
   ],
   "source": [
    "docs = retriever.invoke(\"I want a wine that has fruity nodes\")\n",
    "print(docs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Luxurious, sweet wine with flavors of honey, apricot, and peach', metadata={'color': 'white', 'country': 'France', 'grape': 'Sémillon', 'name': \"Château d'Yquem\", 'rating': 98, 'year': 2015}),\n",
       " Document(page_content='Highly sought-after Pinot Noir with red fruit and earthy notes', metadata={'color': 'red', 'country': 'France', 'grape': 'Pinot Noir', 'name': 'Domaine de la Romanée-Conti', 'rating': 100, 'year': 2018})]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This example specifies a query and a filter\n",
    "retriever.invoke(\"I want a wine that has fruity nodes and has a rating above 97\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- keep top k "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm,\n",
    "    vectorstore,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    "    enable_limit=True,  ### enable limit\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Luxurious, sweet wine with flavors of honey, apricot, and peach', metadata={'color': 'white', 'country': 'France', 'grape': 'Sémillon', 'name': \"Château d'Yquem\", 'rating': 98, 'year': 2015}),\n",
       " Document(page_content='Highly sought-after Pinot Noir with red fruit and earthy notes', metadata={'color': 'red', 'country': 'France', 'grape': 'Pinot Noir', 'name': 'Domaine de la Romanée-Conti', 'rating': 100, 'year': 2018})]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents(\"what are two that have a rating above 97\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Parent Document Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You may want to have small documents, so that their embeddings can most accurately reflect their meaning. If too long, then the embeddings can lose meaning.\n",
    "- You want to have long enough documents that the context of each chunk is retained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader,DirectoryLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 275.25it/s]\n"
     ]
    }
   ],
   "source": [
    "## reference : https://python.langchain.com/docs/modules/data_connection/document_loaders/file_directory\n",
    "data_folder = '/root/workspace/data/Adv_RAG_temp_data'\n",
    "loader = DirectoryLoader(data_folder, glob=\"*.txt\",loader_cls=TextLoader,show_progress=True,use_multithreading=True)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Retrieving full documents : In this mode, we want to retrieve the full documents. Therefore, we only specify a child splitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This text splitter is used to create the child documents\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n",
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"full_documents\", embedding_function=OpenAIEmbeddings()\n",
    ")\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    ")\n",
    "retriever.add_documents(docs, ids=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['34c0989f-e952-41cc-aded-17edfac1f3bf',\n",
       " '277aa046-c08c-4563-a1c9-3b873b3f6ba3']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## we have two docs, so two store ids\n",
    "list(store.yield_keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieve chunks in docs that is related \n",
      "[Document(page_content='Chinese dynasty, which was at once involved in the usual internal and\\nexternal struggles. For the moment, however, the southern region was\\nrelatively at peace, and was accordingly attracting settlers.', metadata={'doc_id': '277aa046-c08c-4563-a1c9-3b873b3f6ba3', 'source': '/root/workspace/data/Adv_RAG_temp_data/chinahistory.txt'}),\n",
      " Document(page_content='North China before its defeat, and resumed these from 932 on; there were\\neven relations with one of the South Chinese states; in the same way,\\nKao-li continuously played one state against the other (M. Rogers _et\\nal_.).', metadata={'doc_id': '277aa046-c08c-4563-a1c9-3b873b3f6ba3', 'source': '/root/workspace/data/Adv_RAG_temp_data/chinahistory.txt'}),\n",
      " Document(page_content='inside China we followed, had for the first time to defend itself\\nagainst views and systems entirely opposed to it; for the Turkish and\\nMongol peoples who ruled northern China brought with them their\\ntraditions of a feudal nobility with privileges of birth and all that\\nthey implied. Thus this period, socially regarded, is especially that of', metadata={'doc_id': '277aa046-c08c-4563-a1c9-3b873b3f6ba3', 'source': '/root/workspace/data/Adv_RAG_temp_data/chinahistory.txt'}),\n",
      " Document(page_content='activities all over China and often monopolized the salt, silver, rice,\\ncotton, silk or tea businesses. In the sixteenth century they had\\nwell-established contacts with smugglers on the Fukien coast and brought\\nforeign goods into the interior. Their home was also close to the main\\ncentres of porcelain production in Kiangsi which was exported to', metadata={'doc_id': '277aa046-c08c-4563-a1c9-3b873b3f6ba3', 'source': '/root/workspace/data/Adv_RAG_temp_data/chinahistory.txt'})]\n"
     ]
    }
   ],
   "source": [
    "## - Let’s now call the vector store search functionality - we should see that it returns small chunks (since we’re storing the small chunks).\n",
    "sub_docs = vectorstore.similarity_search(\"Chinese history\")\n",
    "print('retrieve chunks in docs that is related ')\n",
    "pprint.pprint(sub_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chinese dynasty, which was at once involved in the usual internal and\n",
      "external struggles. For the moment, however, the southern region was\n",
      "relatively at peace, and was accordingly attracting settlers.\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "print(sub_docs[0].page_content)\n",
    "print(len(sub_docs[0].page_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we can retrieve the parent document using the parent document retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "959799"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## so for instance in a news article setting, we can using small chunks for matching and return the entire document \n",
    "retrieved_docs = retriever.get_relevant_documents(\"Chinese history\")\n",
    "len(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ##### A second way to use it is to return a larger chunk rather than the entire document "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This text splitter is used to create the parent documents\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)\n",
    "# This text splitter is used to create the child documents\n",
    "# It should create documents smaller than the parent\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n",
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"split_parents\", embedding_function=OpenAIEmbeddings()\n",
    ")\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    ")\n",
    "\n",
    "retriever.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1004"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## We can see that there are much more than two documents now - these are the larger chunks.\n",
    "len(list(store.yield_keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let’s make sure the underlying vector store still retrieves the small chunks.\n",
    "sub_docs = vectorstore.similarity_search(\"Chinese history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Illustration: Map 3. China in the struggle with the Huns or Hsiung Nu\n",
      "(_roughly 128-100 B.C._)]\n"
     ]
    }
   ],
   "source": [
    "print(sub_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Illustration: Map 3. China in the struggle with the Huns or Hsiung Nu\n",
      "(_roughly 128-100 B.C._)]\n",
      "\n",
      "The first active step taken was to try, in 133 B.C., to capture the\n",
      "head of the Hsiung-nu state, who was called a _shan-yü_ but the\n",
      "_shan-yü_ saw through the plan and escaped. There followed a period of\n",
      "continuous fighting until 119 B.C. The Chinese made countless attacks,\n",
      "without lasting success. But the Hsiung-nu were weakened, one sign of\n",
      "this being that there were dissensions after the death of the _shan-yü_\n",
      "Chün-ch'en, and in 127 B.C. his son went over to the Chinese. Finally\n",
      "the Chinese altered their tactics, advancing in 119 B.C. with a strong\n",
      "army of cavalry, which suffered enormous losses but inflicted serious\n",
      "loss on the Hsiung-nu. After that the Hsiung-nu withdrew farther to the\n",
      "north, and the Chinese settled peasants in the important region of\n",
      "Kansu.\n"
     ]
    }
   ],
   "source": [
    "## with the parent doc retriever to get larger chunks \n",
    "retrieved_docs = retriever.get_relevant_documents(\"Chinese history\")\n",
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  Hybrid Search BM25 & Ensemble Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain_community.vectorstores import FAISS, Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "embedding = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list = [\n",
    "    \"I like apples\",\n",
    "    \"I like oranges\",\n",
    "    \"Apples and oranges are fruits\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the bm25 retriever and faiss retriever\n",
    "bm25_retriever = BM25Retriever.from_texts(doc_list)\n",
    "bm25_retriever.k = 2\n",
    "\n",
    "embedding = OpenAIEmbeddings()\n",
    "faiss_vectorstore = FAISS.from_texts(doc_list, embedding)\n",
    "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- initialize the ensemble retriever : and provide a weighting system \n",
    "- it is often useful when people know exactly what they want to search for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='I like apples'),\n",
       " Document(page_content='Apples and oranges are fruits')]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = ensemble_retriever.get_relevant_documents(\"apples\")\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Contextual Compressors & Filters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
