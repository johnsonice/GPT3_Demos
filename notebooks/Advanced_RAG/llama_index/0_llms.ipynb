{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM standard usage in Llamaindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_path = '../../../.env'\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in environment variables. Please check your .env file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM standard usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## params : https://docs.llamaindex.ai/en/stable/api_reference/llms/openai/\n",
    "llm = OpenAI(model=\"gpt-4o-mini\",\n",
    "             temperature=0.5,\n",
    "             max_tokens=1600,\n",
    "             api_key=os.getenv(\"OPENAI_API_KEY\"), ## default from os.environ\n",
    "             api_base=\"https://api.openai.com/v1\",\n",
    "             #api_version=\"2024-05-13\",\n",
    "             max_retries=3,\n",
    "             timeout=60.0,\n",
    "             reuse_client=True,\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Basic completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paul Graham is an influential computer scientist, entrepreneur, and venture capitalist, best known for co-founding the startup accelerator Y Combinator. He is also a programmer and writer, having authored several essays on topics related to technology, startups, and programming. Graham is known for his insights on the startup ecosystem, and his essays have been widely read in the tech community.\n",
      "\n",
      "In addition to his work with Y Combinator, he created the programming language Arc and has contributed to various projects in the tech industry. His thoughts on entrepreneurship, innovation, and the culture of startups have made him a prominent figure in Silicon Valley.\n",
      "Paul Graham is a prominent computer scientist, entrepreneur, and venture capitalist, best known for co-founding the startup accelerator Y Combinator in 2005. He is also known for his essays on technology, startups, and programming, which have been widely read and influential in the tech community. Graham has a background in computer science and has worked on various projects, including the development of the Lisp programming language and the creation of the online publishing platform, Arc. His insights on entrepreneurship, innovation, and the startup ecosystem have made him a respected figure in Silicon Valley and beyond."
     ]
    }
   ],
   "source": [
    "# non-streaming\n",
    "completion = llm.complete(\"Paul Graham is \")\n",
    "print(completion)\n",
    "# streaming\n",
    "completions = llm.stream_complete(\"Paul Graham is \")\n",
    "for completion in completions:\n",
    "    print(completion.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Chat completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ahoy there, matey! Ye can call me Captain Chatbeard, the most swashbucklin‚Äô chatty pirate on the seven seas! What treasure of knowledge be ye seekin‚Äô today? Arrr! üè¥‚Äç‚ò†Ô∏è‚ú®\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"system\", content=\"You are a pirate with a colorful personality\"\n",
    "    ),\n",
    "    ChatMessage(role=\"user\", content=\"What is your name\"),\n",
    "]\n",
    "resp = llm.chat(messages)\n",
    "print(resp.message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Async chat completion and compare with sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio, asyncio\n",
    "import time\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# async chat completion\n",
    "async def async_chat(messages_list: list[list[ChatMessage]]):\n",
    "    # Create tasks for all messages to run concurrently\n",
    "    tasks = [llm.achat(messages) for messages in messages_list]\n",
    "    # Gather all responses asynchronously\n",
    "    responses = await asyncio.gather(*tasks)\n",
    "    return responses\n",
    "\n",
    "def sync_chat(messages_list: list[list[ChatMessage]]):\n",
    "    # Process messages synchronously\n",
    "    responses = []\n",
    "    for messages in messages_list:\n",
    "        response = llm.chat(messages)\n",
    "        responses.append(response)\n",
    "    return responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "messages_batch = [\n",
    "    [\n",
    "        ChatMessage(\n",
    "            role=\"system\", content=\"You are a helpful assistant\"\n",
    "        ),\n",
    "        ChatMessage(role=\"user\", content=\"Tell me a short story\"),\n",
    "    ],\n",
    "    [\n",
    "        ChatMessage(\n",
    "            role=\"system\", content=\"You are a pirate captain\"\n",
    "        ), \n",
    "        ChatMessage(role=\"user\", content=\"What's your favorite treasure?\"),\n",
    "    ],\n",
    "    [\n",
    "        ChatMessage(\n",
    "            role=\"system\", content=\"You are a chef\"\n",
    "        ),\n",
    "        ChatMessage(role=\"user\", content=\"What's your signature dish?\"),\n",
    "    ]\n",
    "] *3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Synchronous Results:\n",
      "Time taken: 43.57 seconds\n",
      "\n",
      "Asynchronous Results:\n",
      "Time taken: 7.31 seconds\n",
      "\n",
      "Speed comparison: Async was 5.96x faster\n"
     ]
    }
   ],
   "source": [
    "# Compare sync vs async performance\n",
    "# Synchronous execution\n",
    "start_time = time.time()\n",
    "sync_responses = sync_chat(messages_batch)\n",
    "sync_time = time.time() - start_time\n",
    "print(\"\\nSynchronous Results:\")\n",
    "print(f\"Time taken: {sync_time:.2f} seconds\")\n",
    "# for i, response in enumerate(sync_responses, 1):\n",
    "#     print(f\"\\nResponse {i}:\")\n",
    "#     print(response.message.content)\n",
    "\n",
    "# Asynchronous execution\n",
    "start_time = time.time()\n",
    "responses = await async_chat(messages_batch)\n",
    "async_time = time.time() - start_time\n",
    "print(\"\\nAsynchronous Results:\")\n",
    "print(f\"Time taken: {async_time:.2f} seconds\")\n",
    "# for i, response in enumerate(responses, 1):\n",
    "#     print(f\"\\nResponse {i}:\")\n",
    "#     print(response.message.content)\n",
    "\n",
    "print(f\"\\nSpeed comparison: Async was {sync_time/async_time:.2f}x faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Song(BaseModel):\n",
    "    \"\"\"Data model for a song.\"\"\"\n",
    "    title: str\n",
    "    length_seconds: int\n",
    "\n",
    "class Album(BaseModel):\n",
    "    \"\"\"Data model for an album.\"\"\"\n",
    "    name: str\n",
    "    artist: str\n",
    "    songs: List[Song]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- use structured llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Album(name='The Shining Soundtrack', artist='Various Artists', songs=[Song(title='Main Title Theme', length_seconds=120), Song(title='Rocky Mountains', length_seconds=150), Song(title='The Overlook Hotel', length_seconds=180), Song(title='A Good Boy', length_seconds=90), Song(title='The Hedge Maze', length_seconds=200), Song(title='The Blood Elevator', length_seconds=160), Song(title='End Title Theme', length_seconds=130)])\n"
     ]
    }
   ],
   "source": [
    "sllm = llm.as_structured_llm(output_cls=Album)\n",
    "input_msg = ChatMessage.from_str(\"Generate an example album from The Shining\")\n",
    "output = sllm.chat([input_msg])\n",
    "# get actual object\n",
    "output_obj = output.raw\n",
    "pprint(output_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- use structured prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Album(name='The Shining Soundtrack', artist='Various Artists', songs=[Song(title='Main Title', length_seconds=120), Song(title='Rocky Mountains', length_seconds=150), Song(title='The Overlook Hotel', length_seconds=180), Song(title='The Maze', length_seconds=200), Song(title=\"Dee's Theme\", length_seconds=140), Song(title='The Shining (End Title)', length_seconds=160)])\n"
     ]
    }
   ],
   "source": [
    "chat_prompt_tmpl = ChatPromptTemplate(\n",
    "    message_templates=[\n",
    "        ChatMessage.from_str(\n",
    "            \"Generate an example album from The Shining\", role=\"user\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "response = llm.structured_predict(\n",
    "    Album, chat_prompt_tmpl\n",
    ")\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Structured output with in chat - reference: https://docs.llamaindex.ai/en/stable/understanding/extraction/lower_level/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tool use with agents : https://docs.llamaindex.ai/en/stable/examples/agent/openai_agent_parallel_function_calling/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(x: int, y: int) -> int:\n",
    "    \"\"\"Adds two integers together.\"\"\"\n",
    "    return x + y\n",
    "def mystery(x: int, y: int) -> int: \n",
    "    \"\"\"Mystery function that operates on top of two numbers.\"\"\"\n",
    "    return (x + y) * (x + y)\n",
    "\n",
    "def random_int(x: int, y: int):\n",
    "    \"\"\"a random integer between two numbers.\"\"\"\n",
    "    return random.randint(x, y)\n",
    "\n",
    "add_tool = FunctionTool.from_defaults(fn=add)\n",
    "mystery_tool = FunctionTool.from_defaults(fn=mystery)\n",
    "rand_num_tool = FunctionTool.from_defaults(fn=random_int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: mystery with args: {\"x\": 2, \"y\": 9}\n",
      "=== Function Output ===\n",
      "121\n",
      "121\n"
     ]
    }
   ],
   "source": [
    "## predict with a query directly \n",
    "response = llm.predict_and_call(\n",
    "    [add_tool, mystery_tool], \n",
    "    \"Tell me the output of the mystery function on 2 and 9\", \n",
    "    verbose=True\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- select tools only https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/llms/function_calling.py#L18\n",
    "- single turn multiple tools calls with openai agent: https://docs.llamaindex.ai/en/stable/examples/agent/openai_agent_parallel_function_calling/#single-turn-multi-function-calling-openai-agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: \n",
      "Tool calls:\n",
      "tool name: random_int  tool kwargs: {'x': 1, 'y': 100}\n"
     ]
    }
   ],
   "source": [
    "response = llm.chat_with_tools(\n",
    "    [add_tool, mystery_tool,rand_num_tool], \n",
    "    user_msg= \"i will generate a random number between 1 and 100; and also tell me the output of the mystery function on 2 and 9\", \n",
    "    #chat_history: Optional[List[llama_index.core.base.llms.types.ChatMessage]] = None,\n",
    "    verbose=True\n",
    ")\n",
    "print(response) # there is no actually llm response here, just tool calls\n",
    "\n",
    "tool_calls = llm.get_tool_calls_from_response(\n",
    "    response, error_on_no_tool_calls=False\n",
    ")\n",
    "\n",
    "outputs = []\n",
    "print(\"Tool calls:\")\n",
    "for tool_call in tool_calls:\n",
    "    print(\"tool name:\", tool_call.tool_name, \" tool kwargs:\", tool_call.tool_kwargs)\n",
    "    # printout tools selected \n",
    "\n",
    "## looks like it only select one tool call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use other LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- use third openai compatible llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai_like import OpenAILike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_path = '../../../.env'\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "netmind_api_key = os.getenv(\"NETMIND_API_KEY\")\n",
    "if not netmind_api_key:\n",
    "    raise ValueError(\"NETMIND_API_KEY not found in environment variables. Please check your .env file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "netmind_base_url=\"https://api.netmind.ai/inference-api/openai/v1\"\n",
    "\n",
    "netmind_llm = OpenAILike(model=model_name,\n",
    "             temperature=0.5,\n",
    "             max_tokens=1600,\n",
    "             api_key=os.getenv(\"NETMIND_API_KEY\"), \n",
    "             api_base=netmind_base_url,\n",
    "             #api_version=\"2024-05-13\",\n",
    "             max_retries=3,\n",
    "             timeout=60.0,\n",
    "             reuse_client=True,\n",
    "             )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Arrrr, me hearty! Me name be Captain Zingpocket, the most feared and infamous pirate to ever sail the Seven Seas! Me and me trusty parrot, Polly, have been plunderin' and pillagin' for nigh on 20 years, and we've got the treasure to prove it! *winks*\n",
      "\n",
      "Now, what be bringin' ye to these fair waters? Are ye lookin' to join me crew and sail the high seas? Or perhaps ye be wantin' to challenge me to a duel and see who comes out on top? Either way, I be ready for ye! *cracks knuckles* Savvy?\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"system\", content=\"You are a pirate with a colorful personality\"\n",
    "    ),\n",
    "    ChatMessage(role=\"user\", content=\"What is your name\"),\n",
    "]\n",
    "resp = netmind_llm.chat(messages)\n",
    "print(resp.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- use Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.anthropic import Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Anthropic_API_KEY = os.getenv(\"Anthropic_API_KEY\")\n",
    "if not Anthropic_API_KEY:\n",
    "    raise ValueError(\"Anthropic_API_KEY not found in environment variables. Please check your .env file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "claude_llm = Anthropic(model=\"claude-3-opus-20240229\",\n",
    "                       api_key=Anthropic_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: In a small village, a young girl named Lily discovered a mysterious, glowing seed. She planted it in her garden and nurtured it daily. As the seed grew, it transformed into a magnificent tree with shimmering leaves and enchanting flowers. The tree's beauty attracted visitors from far and wide, bringing joy and wonder to all who saw it. Lily realized that the true magic lay not in the tree itself, but in the love and care she had poured into it. From that day on, she continued to spread happiness by sharing the seeds of kindness with everyone she met.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"system\", content=\"You are a pirate with a colorful personality\"\n",
    "    ),\n",
    "    ChatMessage(role=\"user\", content=\"Tell me a shrot story with 100 words\"),\n",
    "]\n",
    "resp = claude_llm.chat(messages)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
